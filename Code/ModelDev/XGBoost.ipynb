{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost    ~Jyosna Philip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score,root_mean_squared_error #Additional scklearn functions\n",
    "from sklearn.model_selection import GroupShuffleSplit,GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv(\"C:\\\\Users\\\\jyosn\\\\Documents\\\\GitHub\\\\Predictive-Maintenance\\\\Datasets\\\\ForModelDev\\\\XTrain.csv\")\n",
    "y_train = pd.read_csv(\"C:\\\\Users\\\\jyosn\\\\Documents\\\\GitHub\\\\Predictive-Maintenance\\\\Datasets\\\\ForModelDev\\\\YTrain.csv\")\n",
    "engine = np.array(pd.read_csv(\"C:\\\\Users\\\\jyosn\\\\Documents\\\\GitHub\\\\Predictive-Maintenance\\\\Datasets\\\\ForModelDev\\\\Engine.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engine=engine.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xg boost -base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss_b = GroupShuffleSplit(n_splits = 1, train_size = 0.7, random_state = 42)\n",
    "train_index_b,val_index_b=next(gss_b.split(x_train, y_train, engine))\n",
    "Y_train=np.asarray(y_train).ravel()\n",
    "xgb_b = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "xgb_b.fit(x_train.iloc[train_index_b,:], Y_train[train_index_b])\n",
    "prediction=xgb_b.predict(x_train.iloc[val_index_b,:])\n",
    "print(r2_score( Y_train[val_index_b],prediction))\n",
    "print(root_mean_squared_error( Y_train[val_index_b],prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb with cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss_outer = GroupShuffleSplit(n_splits = 10, train_size = 0.7, random_state = 42) #n_splits refers to number of folds required\n",
    "r2=[]\n",
    "rmse=[]\n",
    "for i, (train_index, val_index) in enumerate(gss_outer.split(x_train, y_train, Engine)):\n",
    "    xtrain = x_train.iloc[train_index, :] # Train Data from training set (70% split)\n",
    "    ytrain = y_train.iloc[train_index, :]\n",
    "    Y_train=np.asarray(ytrain).ravel()\n",
    "    xval = x_train.iloc[val_index, :] # Validation Data from training set (30% split)\n",
    "    yval = y_train.iloc[val_index, :]\n",
    "    \n",
    "    xgb_cv =XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "    xgb_cv.fit(xtrain, Y_train)\n",
    "    r2.append(r2_score( yval,xgb_cv.predict(xval)))\n",
    "    rmse.append(root_mean_squared_error(yval,xgb_cv.predict(xval)))\n",
    "print('R2: %.3f (%.3f)' % (np.mean(r2), np.std(r2)))\n",
    "print('RMSE: %.3f (%.3f)' % (np.mean(rmse), np.std(rmse)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb with gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss_grid =  GroupShuffleSplit(n_splits = 1, train_size = 0.7, random_state = 42)\n",
    "\n",
    "xgb_grid = XGBRegressor(random_state=42,verbose=1)\n",
    "para={\"learning_rate\":[0.05,0.1,0.2],\"n_estimators\":[70,100,200,300],\"max_depth\":[5,6,7,8],\"min_samples_split\":[150,200],\"criterion\":[\"squared_error\"],\"min_samples_leaf\":[50,80] }\n",
    "train_index_grid,val_index_grid=next(gss_grid.split(x_train, y_train, Engine))\n",
    "# define search\n",
    "search = GridSearchCV(xgb_grid, param_grid=para, scoring=['r2','neg_root_mean_squared_error'], cv=gss_grid, refit='r2')\n",
    "# execute search\n",
    "Y_train=np.asarray(y_train).ravel()\n",
    "result = search.fit(x_train.iloc[train_index_grid,:], Y_train[train_index_grid],groups=Engine[train_index_grid])\n",
    "# get the best performing model fit on the whole training set\n",
    "best_model = result.best_estimator_\n",
    "yhat = best_model.predict(x_train[val_index_grid,:])\n",
    "# evaluate the model\n",
    "r2 = r2_score(Y_train[val_index_grid], yhat)\n",
    "rmse=root_mean_squared_error(Y_train[val_index_grid],yhat)\n",
    "print(\"r2: \",r2)\n",
    "print(\"rmse: \",rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient boosting with gridsearch cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss_grid_cv =  GroupShuffleSplit(n_splits = 10, train_size = 0.7, random_state = 42)\n",
    "\n",
    "xgb_grid_cv =XGBRegressor(random_state=42,verbose=1)\n",
    "para={\"learning_rate\":[0.05,0.1,0.2],\"n_estimators\":[100,200,300,400],\"max_depth\":[5,6,7,8],\"min_samples_split\":[150,200],\"criterion\":[\"squared_error\"],\"min_samples_leaf\":[50,80] }\n",
    "train_index_grid_cv,val_index_grid_cv=next(gss_grid_cv.split(x_train, y_train, Engine))\n",
    "# define search\n",
    "search = GridSearchCV(xgb_grid_cv, param_grid=para, scoring=['r2','neg_root_mean_squared_error'], cv=gss_grid_cv, refit='r2')\n",
    "# execute search\n",
    "Y_train=np.asarray(y_train).ravel()\n",
    "result = search.fit(x_train[train_index_grid_cv,:], Y_train[train_index_grid_cv],groups=Engine[train_index_grid_cv])\n",
    "# get the best performing model fit on the whole training set\n",
    "best_model = result.best_estimator_\n",
    "yhat = best_model.predict(x_train[val_index_grid_cv,:])\n",
    "# evaluate the model\n",
    "r2 = r2_score(Y_train[val_index_grid_cv], yhat)\n",
    "rmse=root_mean_squared_error(Y_train[val_index_grid_cv],yhat)\n",
    "print(\"r2: \",r2)\n",
    "print(\"rmse: \",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(best_model, open('xgb_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradient boosting with nested gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss_outer = GroupShuffleSplit(n_splits = 10, train_size = 0.7, random_state = 42) #n_splits refers to number of folds required\n",
    "\n",
    "# enumerate splits\n",
    "outer_results = {\"r2\":[],\"rmse\":[]}\n",
    "for i,(train_ix, test_ix) in enumerate(gss_outer.split(x_train, y_train, engine)):\n",
    "    # split data\n",
    "    X_train, X_test = x_train.iloc[train_ix, :], x_train.iloc[test_ix, :]\n",
    "    Y_train, Y_test = y_train.iloc[train_ix], y_train.iloc[test_ix]\n",
    "    Engine=engine[train_ix]\n",
    "    Y_train=np.asarray(Y_train).ravel()\n",
    "    Engine=Engine.ravel()\n",
    "    # configure the cross-validation procedure\n",
    "    gss_inner =  GroupShuffleSplit(n_splits = 3, train_size = 0.7, random_state = 42)\n",
    "    # define the model\n",
    "    xgb = XGBRegressor(random_state=42,verbose=1)\n",
    "    para={\"learning_rate\":[0.05,0.1,0.2],\"n_estimators\":[200,300,400],\"max_depth\":[5,6,7,8],\"min_samples_split\":[150,200],\"criterion\":[\"squared_error\"],\"min_samples_leaf\":[50,80] }\n",
    "\n",
    "    # define search\n",
    "    search = GridSearchCV(xgb, param_grid=para, scoring=['r2','neg_root_mean_squared_error'], cv=gss_inner, refit='r2')\n",
    "    # execute search\n",
    "    result = search.fit(X_train, Y_train,groups=Engine)\n",
    "    # get the best performing model fit on the whole training set\n",
    "    best_model = result.best_estimator_\n",
    "    # evaluate model on the hold out dataset\n",
    "    yhat = best_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    r2 = r2_score(Y_test, yhat)\n",
    "    rmse=root_mean_squared_error(Y_test,yhat)\n",
    "    # store the result\n",
    "    outer_results[\"r2\"].append(r2)\n",
    "    outer_results[\"rmse\"].append(rmse)\n",
    "    # report progress\n",
    "    print('>r2=%.3f, est=%.3f, cfg=%s' % (r2, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('R2: %.3f (%.3f)' % (np.mean(outer_results[\"r2\"]), np.std(outer_results[\"r2\"])))\n",
    "print('RMSE: %.3f (%.3f)' % (np.mean(outer_results[\"rmse\"]), np.std(outer_results[\"rmse\"])))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
